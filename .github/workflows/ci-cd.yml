name: CI/CD â€“ build & deploy

on:
  push:
    branches:
      - ci/cd-test
      - main
permissions:
  contents: read
  id-token: write

env:
  IMAGE_NAME: ejahdilan/fastapi-app
  COMPOSE_FILE: ./docker-compose.yml 
  REMOTE_DEPLOY_DIR: /home/churnsvc/churn-app-deploy
  LATEST_TAG: v8

jobs:
  build-and-push:
    name: Build (Buildx) and push Docker image
    runs-on: ubuntu-latest
    outputs:
      image_full: ${{ steps.set-tags.outputs.image_full }}
      image_sha: ${{ steps.set-tags.outputs.image_sha }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        with:
          driver: docker
          buildkitd-flags: --debug

      - name: Cache Docker layers
        uses: actions/cache@v4
        id: docker-cache
        with:
          path: /tmp/.buildx-cache
          key: buildx-cache-${{ github.sha }}
          restore-keys: |
            buildx-cache-

      - name: Set image tags
        id: set-tags
        run: |
          SHORT_SHA=${GITHUB_SHA::8}
          IMAGE_SHA_TAG=${IMAGE_NAME}:${SHORT_SHA}
          IMAGE_LATEST_TAG=${IMAGE_NAME}:${LATEST_TAG}
          echo "image_sha=${IMAGE_SHA_TAG}" >> $GITHUB_OUTPUT
          echo "image_full=${IMAGE_LATEST_TAG}" >> $GITHUB_OUTPUT

      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          registry: docker.io
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Build and push image (multi-platform ready)
        uses: docker/build-push-action@v6
        with:
          context: . 
          file: ./dockerfile 
          push: true
          tags: |
            ${{ steps.set-tags.outputs.image_sha }}
            ${{ steps.set-tags.outputs.image_full }}
          cache-from: type=inline 
          cache-to: type=inline,mode=max
          builder: default

  deploy-to-azure-vm:
    name: Copy compose and deploy to Azure VM
    runs-on: ubuntu-latest
    needs: build-and-push
    steps:
      - name: Checkout repo (to get compose file)
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Prepare known_hosts entry for SSH
        id: known-hosts
        run: |
          mkdir -p ~/.ssh
          ssh-keyscan -H ${{ secrets.AZURE_SSH_HOST }} >> ~/.ssh/known_hosts
          chmod 600 ~/.ssh/known_hosts
        env:
           SSH_HOST: ${{ secrets.AZURE_SSH_HOST }}

      - name: Copy docker-compose to Azure VM
        uses: appleboy/scp-action@v1.0.0
        with:
          host: ${{ secrets.AZURE_SSH_HOST }}
          username: ${{ secrets.AZURE_SSH_USER }}
          port: ${{ secrets.AZURE_SSH_PORT || 22 }}
          key: ${{ secrets.AZURE_SSH_PRIVATE_KEY }}
          source: "./docker-compose.yml"
          target: ${{ env.REMOTE_DEPLOY_DIR }}
          strip_components: 0
        # Note: source supports globbing; copy additional files if needed

      - name: Download latest model and restart service on remote VM
        uses: appleboy/ssh-action@v1
        env:
          AZURE_STORAGE_CONNECTION_STRING: ${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}
          CONTAINER_NAME: ${{ secrets.CONTAINER_NAME }}
          MODEL_DIR: ${{ env.REMOTE_DEPLOY_DIR }}/models
          REMOTE_DIR: ${{ env.REMOTE_DEPLOY_DIR }}
        with:
          host: ${{ secrets.AZURE_SSH_HOST }}
          username: ${{ secrets.AZURE_SSH_USER }}
          port: ${{ secrets.AZURE_SSH_PORT}}
          key: ${{ secrets.AZURE_SSH_PRIVATE_KEY }}
          envs: AZURE_STORAGE_CONNECTION_STRING,CONTAINER_NAME,MODEL_DIR,REMOTE_DIR
          script: |
            set -euo pipefail

            # Install azure-storage-blob Python package for the current user
            python3 -m pip install --user --break-system-packages azure-storage-blob

            # Download the latest model.pkl blob from Azure Blob Storage
            python3 - <<'PYCODE'
            import os
            from azure.storage.blob import BlobServiceClient

            model_dir = os.getenv("MODEL_DIR")
            os.makedirs(model_dir, exist_ok=True)

            conn_str = os.getenv("AZURE_STORAGE_CONNECTION_STRING")
            container_name = os.getenv("CONTAINER_NAME", "mlflow")
            target_dir = os.getenv("MODEL_DIR", "models")

            os.makedirs(target_dir, exist_ok=True)

            blob_service_client = BlobServiceClient.from_connection_string(conn_str)
            container_client = blob_service_client.get_container_client(container_name)

            # blobs = [b for b in container_client.list_blobs() if b.name.endswith("model.pkl")]

            blobs = [
                      b for b in container_client.list_blobs()
                      if b.name.endswith("model.pkl")
                      and len(b.name.split('/')) > 1
                      and b.name.split('/')[1].startswith('1')
                    ]
            
            if not blobs:
                raise SystemExit("No model.pkl found in container.")

            blobs.sort(key=lambda b: b.last_modified)

            latest_blobs = blobs[-2:]

            for latest_blob in latest_blobs:

                parts = latest_blob.name.split('/')
                try:
                    model_id = parts[2]

                except IndexError:
                    raise SystemExit(f"Unexpected blob path structure: {latest_blob.name}")

                local_path = os.path.join(target_dir, f"{model_id}_model.pkl")

                print(f"Downloading latest model: {latest_blob.name}")
                print(f"Model ID: {model_id}")
                print(f"Saving to: {local_path}")

                blob_client = container_client.get_blob_client(latest_blob.name)

                with open(local_path, "wb") as f:
                    f.write(blob_client.download_blob().readall())

                print(f"Model {model_id} downloaded successfully to {local_path}")
            PYCODE

            # # Navigate to remote deploy directory and rebuild/restart containers
            # cd "$REMOTE_DIR"
            # if command -v docker compose >/dev/null 2>&1; then
            #   docker compose up -d --build --remove-orphans
            # else
            #   echo "docker compose not found, cannot restart services."
            #   exit 1
            # fi


      - name: Pull image and deploy on remote VM (docker-compose)
        uses: appleboy/ssh-action@v1
        env:
          REMOTE_DIR: ${{ env.REMOTE_DEPLOY_DIR }}
          DOCKERHUB_USERNAME: ${{ secrets.DOCKERHUB_USERNAME }}
          DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
        with:
          host: ${{ secrets.AZURE_SSH_HOST }}
          username: ${{ secrets.AZURE_SSH_USER }}
          port: ${{ secrets.AZURE_SSH_PORT}}
          key: ${{ secrets.AZURE_SSH_PRIVATE_KEY }}
          envs: REMOTE_DIR,DOCKERHUB_USERNAME,DOCKERHUB_TOKEN
          script: |
            set -euo pipefail

            # ensure directory
            mkdir -p "$REMOTE_DIR" || true
            cd "$REMOTE_DIR"

            # login to docker hub (non-interactive)
            echo "$DOCKERHUB_TOKEN" | docker login --username "$DOCKERHUB_USERNAME" --password-stdin

            # Pull and restart services
            if command -v docker compose >/dev/null 2>&1; then
              docker compose pull --ignore-pull-failures
              docker compose up -d --remove-orphans
            else
              echo "docker compose not found, cannot deploy services."
              exit 1
            fi

            # optional: cleanup dangling images
            docker image prune -f || true
